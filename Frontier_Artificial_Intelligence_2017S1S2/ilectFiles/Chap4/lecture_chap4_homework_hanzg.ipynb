{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 第4回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 課題. MNISTデータセットを多層パーセプトロン(MLP)で学習せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 注意\n",
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データはtrain_X, train_y, テストデータはtest_Xで与えられます\n",
    "    - train_Xとtrain_yをtrain_X, train_yとvalid_X, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - test_Xに対して予想ラベルpred_yを作り, homework関数の戻り値としてください\\\n",
    "- pred_yのtest_yに対する精度(F値)で評価します\n",
    "- 全体の実行時間がiLect上で60分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください (必要なものは全てhomework関数に入れてください)\n",
    "- 解答提出時には Answer Cell の内容のみを提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- MLPの実装にTensorflowなどのライブラリを使わないでください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ヒント\n",
    "- 出力yはone-of-k表現\n",
    "- 最終層の活性化関数はソフトマックス関数, 誤差関数は多クラス交差エントロピー\n",
    "- 最終層のデルタは教科書参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "次のセルのhomework関数を完成させて提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Answer Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    return exp_x / sum_exp_x # WRITE ME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, t):\n",
    "    return -np.sum(t*np.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_entropy_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-066fe3b807a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_entropy_error' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.array([0.1,0.6,0.3])\n",
    "t = np.array([0,1,0])\n",
    "cross_entropy_error(y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082562376599072"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(784, 100)).astype('float32')\n",
    "b1 = np.zeros(100).astype('float32')\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(100, 10)).astype('float32')\n",
    "b2 = np.zeros(10).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=0.001):\n",
    "    global W1, b1, W2, b2\n",
    "    x = x[np.newaxis,:]\n",
    "    l = np.zeros(10)\n",
    "    l[t] = 1\n",
    "    t = l\n",
    "     \n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    y = z2\n",
    "#     \n",
    "    \n",
    "    cost = cross_entropy_error(y,t)\n",
    "#     cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    delta_2 = y - t # Layer2 delta\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T) # Layer1 delta\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps*dW1\n",
    "    b1 = b1 - eps*db1\n",
    "    \n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps*dW2\n",
    "    b2 = b2 - eps*db2\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "    x = x[np.newaxis,:]\n",
    "    l = np.zeros(10)\n",
    "    l[t] = 1\n",
    "    t = l\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    y = z2   \n",
    "    # Test Cost\n",
    "    cost = -np.sum(t*log(y))\n",
    "    return cost, y.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    eps= 1.0\n",
    "    cost = 0\n",
    "    cost_array = []\n",
    "    pred_y = []\n",
    "    length = train_X.shape[0]\n",
    "    epoch = 100\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        for x, l in zip(train_X, train_y):\n",
    "            x = x[np.newaxis, :]\n",
    "            t = np.zeros(10)\n",
    "            t[l] = 1\n",
    "    #         l = l[np.newaxis, :]\n",
    "\n",
    "            # Layer1 weights\n",
    "            W1 = np.random.uniform(low=-0.08, high=0.08, size=(784, 200)).astype('float32')\n",
    "            b1 = np.zeros(100).astype('float32')\n",
    "\n",
    "            W2 = np.random.uniform(low=-0.08, high=0.08, size=(200, 10)).astype('float32')\n",
    "            b2 = np.zeros(10).astype('float32')\n",
    "\n",
    "            u1 = np.matmul(x, W1) + b1\n",
    "            z1 = sigmoid(u1)\n",
    "\n",
    "            # Forward Propagation Layer2\n",
    "            u2 = np.matmul(z1, W2) + b2\n",
    "            z2 = softmax(u2)\n",
    "\n",
    "            # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "            y = z2   \n",
    "            cost = cross_entropy(y,t) \n",
    "            delta_2 = y - t # Layer2 delta\n",
    "    #         delta_2 = t - y # Layer2 delta\n",
    "            delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T) # Layer1 delta\n",
    "\n",
    "            # Update Parameters Layer1\n",
    "    #         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "            dW1 = np.matmul(x.T, delta_1)\n",
    "            db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "            W1 = W1 - eps*dW1\n",
    "            b1 = b1 - eps*db1\n",
    "\n",
    "            # Update Parameters Layer2\n",
    "            dW2 = np.matmul(z1.T, delta_2)\n",
    "            db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "            W2 = W2 - eps*dW2\n",
    "            b2 = b2 - eps*db2\n",
    "\n",
    "        cost_array = np.append(cost_array, cost)\n",
    "#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        \n",
    "    for test in test_X:\n",
    "        u1 = np.matmul(x, W1) + b1\n",
    "        z1 = sigmoid(u1)\n",
    "        y = softmax(z1)\n",
    "        pred = np.argmax(y)\n",
    "#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        pred_y = np.append(pred_y, pred)\n",
    "    plt.plot(np.arange(epoch),cost_array)\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "ilect": {
     "is_homework": true
    }
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    # WRITE ME!\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 以下のvalidate_homework関数を用いてエラーが起きないか動作確認をして下さい。\n",
    "- 提出に際して、score_homework関数で60分で実行が終わることを確認して下さい。\n",
    "- 評価は以下のscore_homework関数で行われますが、random_stateの値は変更されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checker Cell (for student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "ilect": {
     "course_id": 4,
     "course_rank": 4,
     "is_evaluation": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                               mnist.target.astype('int32'), random_state=42)\n",
    "\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    return train_test_split(mnist_X, mnist_y,\n",
    "                test_size=0.2,\n",
    "                random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    \n",
    "#     train_X = train_X[np.where(train_y==1)]\n",
    "#     test_X = test_X[np.where(test_y==1)]\n",
    "#     train_y = train_y[np.where(train_y==1)]\n",
    "#     test_y = test_y[np.where(test_y==1)]\n",
    "    \n",
    "    m = 100\n",
    "    n = 100\n",
    "    # validate for small dataset\n",
    "    train_X_mini = train_X[:m]\n",
    "    train_y_mini = train_y[:m]\n",
    "    test_X_mini = test_X[:n]\n",
    "    test_y_mini = test_y[:n]\n",
    "\n",
    "#     train_X_mini = train_X[:100]\n",
    "#     train_y_mini = train_y[:100]\n",
    "#     test_X_mini = test_X[:100]\n",
    "#     test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    print(f1_score(test_y_mini, pred_y, average='macro'))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(test_y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#liu\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def deriv_softmax(x):\n",
    "    return softmax(x)*(1-softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N0 = 784\n",
    "N1 = 300\n",
    "N2 = 100\n",
    "N3 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#liu\n",
    "# Layer1 weights\n",
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(784, 300)).astype('float32')\n",
    "b1 = np.zeros(300).astype('float32')\n",
    "\n",
    "# Layer2 weights\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(300, 100)).astype('float32')\n",
    "b2 = np.zeros(100).astype('float32')\n",
    "\n",
    "# Layer3 weights\n",
    "W3 = np.random.uniform(low=-0.08, high=0.08, size=(100, 10)).astype('float32')\n",
    "b3 = np.zeros(10).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(N0, N1)).astype('float32')\n",
    "b1 = np.zeros(N1).astype('float32')\n",
    "\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(N1, N2)).astype('float32')\n",
    "b2 = np.zeros(N2).astype('float32')\n",
    "\n",
    "W3 = np.random.uniform(low=-0.08, high=0.08, size=(N2, N3)).astype('float32')\n",
    "b3 = np.zeros(N3).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=0.2):\n",
    "    global W1, b1, W2, b2, W3, b3  # to access variables that defined outside of this function.\n",
    "\n",
    "    y_tmp = np.zeros(10).reshape(1, 10)\n",
    "    y_tmp[0,t] = 1.0\n",
    "    train_y=y_tmp\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "\n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(u2)\n",
    "    \n",
    "    # Forward Propagation Layer3\n",
    "    u3 = np.matmul(z2, W3) + b3\n",
    "    z3 = softmax(u3)\n",
    "\n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    y = z3\n",
    "    # cost = np.sum(-t * np.log(y) - (1 - t) * np.log(1 - y))\n",
    "    cost = -np.log(y[0,t])\n",
    "    \n",
    "    delta_3 = (y-train_y)\n",
    "    delta_2 = deriv_sigmoid(u2) * np.matmul(delta_3, W3.T)\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T)  # Layer1 delta\n",
    "\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps * dW1\n",
    "    b1 = b1 - eps * db1\n",
    "\n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps * dW2\n",
    "    b2 = b2 - eps * db2\n",
    "    \n",
    "    # Update Parameters Layer3\n",
    "    dW3 = np.matmul(z2.T, delta_3)\n",
    "    db3 = np.matmul(np.ones(len(z2)), delta_3)\n",
    "    W3 = W3 - eps * dW3\n",
    "    b3 = b3 - eps * db3\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#liu\n",
    "def train(x, t, eps=0.2):\n",
    "    global W1, b1, W2, b2,W3,b3  # to access variables that defined outside of this function.\n",
    "\n",
    "    y_tmp = np.zeros(10).reshape(1, 10)\n",
    "    y_tmp[0,t] = 1.0\n",
    "    train_y=y_tmp\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "\n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(u2)\n",
    "\n",
    "    u3 = np.matmul(z2,W3) + b3\n",
    "    z3 = softmax(u3)\n",
    "\n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    y = z3\n",
    "    # cost = np.sum(-t * np.log(y) - (1 - t) * np.log(1 - y))\n",
    "    cost = -np.log(y[0,t])\n",
    "    \n",
    "    delta_3 = (y-train_y)\n",
    "    delta_2 = deriv_sigmoid(u2) * np.matmul(delta_3, W3.T)  # Layer2 delta\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T)  # Layer1 delta\n",
    "\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps * dW1\n",
    "    b1 = b1 - eps * db1\n",
    "\n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps * dW2\n",
    "    b2 = b2 - eps * db2\n",
    "\n",
    "    # Update Parameters Layer3\n",
    "    dW3 = np.matmul(z2.T, delta_3)\n",
    "    db3 = np.matmul(np.ones(len(z2)), delta_3)\n",
    "    W3 = W3 - eps * dW3\n",
    "    b3 = b3 - eps * db3\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(x):\n",
    "\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(u2)\n",
    "    \n",
    "    # Forward Propagation Layer3\n",
    "    u3 = np.matmul(z2, W3) + b3\n",
    "    z3 = softmax(u3)\n",
    "    \n",
    "    y = np.argmax(z3, axis = 1)\n",
    "    \n",
    "#     cost = -np.sum(l*log(y))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#liu\n",
    "def test(x):\n",
    "\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "\n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(u2)\n",
    "\n",
    "    u3 = np.matmul(z2,W3) + b3\n",
    "    z3 = softmax(u3)\n",
    "    y =[]\n",
    "    for one in z3:\n",
    "        y.append(np.argmax(one))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    cost_array = []\n",
    "    cost_sum = []\n",
    "    for epoch in range(30):\n",
    "        # Online Learning\n",
    "        for x, y in zip(train_X, train_y):\n",
    "            x = x[np.newaxis, :]\n",
    "#             y = y[np.newaxis, :]\n",
    "            cost = train(x, y)\n",
    "            cost_sum = np.append(cost_sum, cost)\n",
    "        cost = np.sum(cost_sum)\n",
    "        cost_array = np.append(cost_array, cost)\n",
    "    pred_y = test(test_X)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#liu\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def homework(train_X, train_y, test_X):\n",
    "    cost_plt =[]\n",
    "    for epoch in range(30):\n",
    "    # Online Learning\n",
    "        cost_arry = []\n",
    "        for x, y in zip(train_X, train_y):\n",
    "            x = x.reshape(1,len(x))\n",
    "            cost = train(x, y)\n",
    "            cost_arry.append(cost)\n",
    "            \n",
    "        cost_plt.append(np.sum(cost_arry))\n",
    "#         print cost_plt\n",
    "    plt.plot(cost_plt)\n",
    "    pred_y = test(test_X)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#liu\n",
    "def validate_homework():\n",
    "    import time\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "\n",
    "    # validate for small dataset\n",
    "    train_X_mini = train_X[:100]\n",
    "    train_y_mini = train_y[:100]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "\n",
    "    start = time.time()\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    print(f1_score(test_y_mini, pred_y, average='macro'))\n",
    "    elapsed_time = time.time() - start\n",
    "    print ('elapsed_time:', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699556277056\n",
      "elapsed_time: 3.379586935043335\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEbCAYAAACV0PCVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYhIRlAgiYhIRvj/WGfD+Tv+up8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1136dec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validate_homework()\n",
    "# score_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Layer1 weights\n",
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(784, 100)).astype('float32')\n",
    "b1 = np.zeros(100).astype('float32')\n",
    "\n",
    "# Layer2 weights\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(100,10)).astype('float32')\n",
    "b2 = np.zeros(10).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = np.zeros(10)\n",
    "l[t] = 1\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=1.0):\n",
    "    global W1, b1, W2, b2 # to access variables that defined outside of this function.\n",
    "    \n",
    "    y_tmp = np.zeros(10).reshape(1, 10)\n",
    "    y_tmp[0,t] = 1.0\n",
    "    t=y_tmp\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "#     y = z2\n",
    "#     t = (np.zeros(10)[t] = 1)\n",
    "#     t[]\n",
    "    \n",
    "    cost = cross_entropy_error(y,t)\n",
    "#     cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    delta_2 = y - t # Layer2 delta\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T) # Layer1 delta\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps*dW1\n",
    "    b1 = b1 - eps*db1\n",
    "    \n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps*dW2\n",
    "    b2 = b2 - eps*db2\n",
    "\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    y = argmax(z2, axis = 1)\n",
    "    \n",
    "#     cost = -np.sum(l*log(y))\n",
    "    return y\n",
    "#     return cost, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=0.2):\n",
    "    global W1, b1, W2, b2  # to access variables that defined outside of this function.\n",
    "\n",
    "    y_tmp = np.zeros(10).reshape(1, 10)\n",
    "    y_tmp[0,t] = 1.0\n",
    "    train_y=y_tmp\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "\n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "\n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    y = z2\n",
    "    # cost = np.sum(-t * np.log(y) - (1 - t) * np.log(1 - y))\n",
    "    cost = -np.log(y[0,t])\n",
    "    \n",
    "    delta_2 = (y-train_y)\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T)  # Layer1 delta\n",
    "\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps * dW1\n",
    "    b1 = b1 - eps * db1\n",
    "\n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps * dW2\n",
    "    b2 = b2 - eps * db2\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-40746fecdd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Online Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "# Epoch\n",
    "cost_array = []\n",
    "cost_sum = []\n",
    "for epoch in range(100):\n",
    "    # Online Learning\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        x = x[np.newaxis, :]\n",
    "        y = y[np.newaxis, :]\n",
    "        cost = train(x, y)\n",
    "        cost_sum = np.append(cost_sum, cost)\n",
    "    pred_y = test(test_X, test_y)\n",
    "    cost = np.sum(cost_sum)\n",
    "    cost_array = np.append(cost_array, cost)\n",
    "\n",
    "print(cost)\n",
    "print(pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
