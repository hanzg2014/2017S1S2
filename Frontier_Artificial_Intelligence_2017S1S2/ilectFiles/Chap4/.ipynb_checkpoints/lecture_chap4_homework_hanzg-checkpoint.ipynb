{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 第4回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 課題. MNISTデータセットを多層パーセプトロン(MLP)で学習せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 注意\n",
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データはtrain_X, train_y, テストデータはtest_Xで与えられます\n",
    "    - train_Xとtrain_yをtrain_X, train_yとvalid_X, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - test_Xに対して予想ラベルpred_yを作り, homework関数の戻り値としてください\\\n",
    "- pred_yのtest_yに対する精度(F値)で評価します\n",
    "- 全体の実行時間がiLect上で60分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください (必要なものは全てhomework関数に入れてください)\n",
    "- 解答提出時には Answer Cell の内容のみを提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- MLPの実装にTensorflowなどのライブラリを使わないでください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ヒント\n",
    "- 出力yはone-of-k表現\n",
    "- 最終層の活性化関数はソフトマックス関数, 誤差関数は多クラス交差エントロピー\n",
    "- 最終層のデルタは教科書参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "次のセルのhomework関数を完成させて提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Answer Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    return exp_x / sum_exp_x # WRITE ME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, t):\n",
    "    return -np.sum(t*np.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_entropy_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-066fe3b807a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_entropy_error' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.array([0.1,0.6,0.3])\n",
    "t = np.array([0,1,0])\n",
    "cross_entropy_error(y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082562376599072"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(784, 100)).astype('float32')\n",
    "b1 = np.zeros(100).astype('float32')\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(100, 10)).astype('float32')\n",
    "b2 = np.zeros(10).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=0.001):\n",
    "    global W1, b1, W2, b2\n",
    "    x = x[np.newaxis,:]\n",
    "    l = np.zeros(10)\n",
    "    l[t] = 1\n",
    "    t = l\n",
    "     \n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    y = z2\n",
    "#     \n",
    "    \n",
    "    cost = cross_entropy_error(y,t)\n",
    "#     cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    delta_2 = y - t # Layer2 delta\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T) # Layer1 delta\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps*dW1\n",
    "    b1 = b1 - eps*db1\n",
    "    \n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps*dW2\n",
    "    b2 = b2 - eps*db2\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "    x = x[np.newaxis,:]\n",
    "    l = np.zeros(10)\n",
    "    l[t] = 1\n",
    "    t = l\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    y = z2   \n",
    "    # Test Cost\n",
    "    cost = -np.sum(t*log(y))\n",
    "    return cost, y.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    eps= 1.0\n",
    "    cost = 0\n",
    "    cost_array = []\n",
    "    pred_y = []\n",
    "    length = train_X.shape[0]\n",
    "    epoch = 100\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        for x, l in zip(train_X, train_y):\n",
    "            x = x[np.newaxis, :]\n",
    "            t = np.zeros(10)\n",
    "            t[l] = 1\n",
    "    #         l = l[np.newaxis, :]\n",
    "\n",
    "            # Layer1 weights\n",
    "            W1 = np.random.uniform(low=-0.08, high=0.08, size=(784, 200)).astype('float32')\n",
    "            b1 = np.zeros(100).astype('float32')\n",
    "\n",
    "            W2 = np.random.uniform(low=-0.08, high=0.08, size=(200, 10)).astype('float32')\n",
    "            b2 = np.zeros(10).astype('float32')\n",
    "\n",
    "            u1 = np.matmul(x, W1) + b1\n",
    "            z1 = sigmoid(u1)\n",
    "\n",
    "            # Forward Propagation Layer2\n",
    "            u2 = np.matmul(z1, W2) + b2\n",
    "            z2 = softmax(u2)\n",
    "\n",
    "            # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "            y = z2   \n",
    "            cost = cross_entropy(y,t) \n",
    "            delta_2 = y - t # Layer2 delta\n",
    "    #         delta_2 = t - y # Layer2 delta\n",
    "            delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T) # Layer1 delta\n",
    "\n",
    "            # Update Parameters Layer1\n",
    "    #         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "            dW1 = np.matmul(x.T, delta_1)\n",
    "            db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "            W1 = W1 - eps*dW1\n",
    "            b1 = b1 - eps*db1\n",
    "\n",
    "            # Update Parameters Layer2\n",
    "            dW2 = np.matmul(z1.T, delta_2)\n",
    "            db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "            W2 = W2 - eps*dW2\n",
    "            b2 = b2 - eps*db2\n",
    "\n",
    "        cost_array = np.append(cost_array, cost)\n",
    "#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        \n",
    "    for test in test_X:\n",
    "        u1 = np.matmul(x, W1) + b1\n",
    "        z1 = sigmoid(u1)\n",
    "        y = softmax(z1)\n",
    "        pred = np.argmax(y)\n",
    "#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        pred_y = np.append(pred_y, pred)\n",
    "    plt.plot(np.arange(epoch),cost_array)\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "ilect": {
     "is_homework": true
    }
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    # WRITE ME!\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 以下のvalidate_homework関数を用いてエラーが起きないか動作確認をして下さい。\n",
    "- 提出に際して、score_homework関数で60分で実行が終わることを確認して下さい。\n",
    "- 評価は以下のscore_homework関数で行われますが、random_stateの値は変更されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checker Cell (for student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "ilect": {
     "course_id": 4,
     "course_rank": 4,
     "is_evaluation": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                               mnist.target.astype('int32'), random_state=42)\n",
    "\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    return train_test_split(mnist_X, mnist_y,\n",
    "                test_size=0.2,\n",
    "                random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    \n",
    "#     train_X = train_X[np.where(train_y==1)]\n",
    "#     test_X = test_X[np.where(test_y==1)]\n",
    "#     train_y = train_y[np.where(train_y==1)]\n",
    "#     test_y = test_y[np.where(test_y==1)]\n",
    "    \n",
    "    m = 100\n",
    "    n = 100\n",
    "    # validate for small dataset\n",
    "    train_X_mini = train_X[:m]\n",
    "    train_y_mini = train_y[:m]\n",
    "    test_X_mini = test_X[:n]\n",
    "    test_y_mini = test_y[:n]\n",
    "\n",
    "#     train_X_mini = train_X[:100]\n",
    "#     train_y_mini = train_y[:100]\n",
    "#     test_X_mini = test_X[:100]\n",
    "#     test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    print(f1_score(test_y_mini, pred_y, average='macro'))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(test_y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    cost_array = []\n",
    "    cost_sum = []\n",
    "    for epoch in range(100):\n",
    "        # Online Learning\n",
    "        for x, y in zip(train_X, train_y):\n",
    "            x = x[np.newaxis, :]\n",
    "#             y = y[np.newaxis, :]\n",
    "            cost = train(x, y)\n",
    "            cost_sum = np.append(cost_sum, cost)\n",
    "        cost = np.sum(cost_sum)\n",
    "        cost_array = np.append(cost_array, cost)\n",
    "    pred_y = test(test_X)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(x):\n",
    "\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    y = np.argmax(z2, axis = 1)\n",
    "    \n",
    "#     cost = -np.sum(l*log(y))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1b587794855b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate_homework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# score_homework()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c4dec81a8d3d>\u001b[0m in \u001b[0;36mvalidate_homework\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#     test_y_mini = test_y[:100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhomework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;31m#     from IPython.core.debugger import Pdb; Pdb().set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cfb77f5c81e8>\u001b[0m in \u001b[0;36mhomework\u001b[0;34m(train_X, train_y, test_X)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#             y = y[np.newaxis, :]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mcost_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a68da195f767>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, t, eps)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Forward Propagation Layer1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'W1' is not defined"
     ]
    }
   ],
   "source": [
    "validate_homework()\n",
    "# score_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Layer1 weights\n",
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(784, 100)).astype('float32')\n",
    "b1 = np.zeros(100).astype('float32')\n",
    "\n",
    "# Layer2 weights\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(100,10)).astype('float32')\n",
    "b2 = np.zeros(10).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = np.zeros(10)\n",
    "l[t] = 1\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=1.0):\n",
    "    global W1, b1, W2, b2 # to access variables that defined outside of this function.\n",
    "    \n",
    "    y_tmp = np.zeros(10).reshape(1, 10)\n",
    "    y_tmp[0,t] = 1.0\n",
    "    t=y_tmp\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "#     y = z2\n",
    "#     t = (np.zeros(10)[t] = 1)\n",
    "#     t[]\n",
    "    \n",
    "    cost = cross_entropy_error(y,t)\n",
    "#     cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    delta_2 = y - t # Layer2 delta\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T) # Layer1 delta\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps*dW1\n",
    "    b1 = b1 - eps*db1\n",
    "    \n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps*dW2\n",
    "    b2 = b2 - eps*db2\n",
    "\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    \n",
    "    y = argmax(z2, axis = 1)\n",
    "    \n",
    "#     cost = -np.sum(l*log(y))\n",
    "    return y\n",
    "#     return cost, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=0.2):\n",
    "    global W1, b1, W2, b2  # to access variables that defined outside of this function.\n",
    "\n",
    "    y_tmp = np.zeros(10).reshape(1, 10)\n",
    "    y_tmp[0,t] = 1.0\n",
    "    train_y=y_tmp\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "\n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "\n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    y = z2\n",
    "    # cost = np.sum(-t * np.log(y) - (1 - t) * np.log(1 - y))\n",
    "    cost = -np.log(y[0,t])\n",
    "    \n",
    "    delta_2 = (y-train_y)\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T)  # Layer1 delta\n",
    "\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps * dW1\n",
    "    b1 = b1 - eps * db1\n",
    "\n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps * dW2\n",
    "    b2 = b2 - eps * db2\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-40746fecdd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Online Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "# Epoch\n",
    "cost_array = []\n",
    "cost_sum = []\n",
    "for epoch in range(100):\n",
    "    # Online Learning\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        x = x[np.newaxis, :]\n",
    "        y = y[np.newaxis, :]\n",
    "        cost = train(x, y)\n",
    "        cost_sum = np.append(cost_sum, cost)\n",
    "    pred_y = test(test_X, test_y)\n",
    "    cost = np.sum(cost_sum)\n",
    "    cost_array = np.append(cost_array, cost)\n",
    "\n",
    "print(cost)\n",
    "print(pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
